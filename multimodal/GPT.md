# 参考

https://zhuanlan.zhihu.com/p/350017443  
强烈推荐： https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756

# GPT 1

https://static.aminer.cn/upload/pdf/1319/1601/76/5f8eab579e795e9e76f6f6a0_0.pdf
Improving Language Understanding by Generative Pre-Training

在GPT-1之前（和ELMo同一年），传统的NLP模型往往使用大量的数据对有监督的模型进行任务相关的模型训练,这必然存在非常多问题。

GPT-1的思想是先通过在无标签的数据上学习一个生成式的语言模型，然后再根据特定任务进行微调，处理的有监督任务包括

- 自然语言推理（Natural Language Inference 或者 Textual Entailment）：判断两个句子是包含关系（entailment），矛盾关系（contradiction），或者中立关系（neutral）；
- 问答和常识推理（Question answering and commonsense reasoning）：类似于多选题，输入一个文章，一个问题以及若干个候选答案，输出为每个答案的预测概率；
- 语义相似度（Semantic Similarity）：判断两个句子是否语义上是相关的；
- 分类（Classification）：判断输入文本是指定的哪个类别。

将无监督学习作为有监督模型的预训练目标，因此叫做生成式预训练（Generative Pre-training，GPT）。

GPT-1的训练分为无监督的预训练和有监督的模型微调，当时还是需要微调的。虽然这一套在 CV 里面非常成熟，但是在 NLP 领域当年还是不行。

**(1) 无监督预训练**
这个部分就是常规的 next token prediction，即给定前面的 token，预测下一个 token。 假设词汇表长度是 768，那么在 decoder 隐含层输出后接一个 embedding 矩阵，将其输出维度映射为 768，然后进行 loss 计算。

**(2) 有监督微调**
这里的微调就是和 CV 里面一样，假设是分类任务，那么 decoder 输出后再接一个新的分类权重矩阵，假设类别是3，那么输出维度就是 3，然后计算 loss。
注意：有监督微调并不是我们现在常说的 next token prediction。

但是作者实际微调时候发现，如果将原先的 next token prediction head 保留并作为辅助 loss 分支，效果更好。

<div align=center>
<img src="https://github.com/QwenLM/Qwen-7B/assets/17425982/c260f041-f3ab-4a2c-8000-7292bab49df2"/>
</div>

左边从下到上，text prediction 就是无监督预训练流程。右边从下到上，task classification 就是有监督分类微调流程(微调时候也要用 text prediction 分支作为辅助，此时只是分隔符的嵌入值才可训练的，其余部分不训练)。

为了确保微调和预训练时候输入保持一致。作者将不同任务的输入构造成一个统一的文本模板，如右图所示。

从上面可以看出，一次预训练后，对于不同的任务都需要再进行一次微调，并且 3 分类和 4 分类情况下也要分别微调两次，和 CV 里面的情况一样。

GPT-1 在未经微调的任务上虽然也有一定效果，但是其泛化能力远远低于经过微调的有监督任务，说明了 GPT-1 只是一个简单的领域专家，而非通用的语言学家。

# GPT2

Language models are unsupervised multitask learners
https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
https://huggingface.co/gpt2

GPT-2是一个在自监督方式下，以非常大的英文数据语料为基础进行预训练的Transformer模型。这意味着它仅仅在原始文本上进行预训练，没有以任何方式由人类进行标注（这就是为什么它可以使用大量的公开可用数据），而是通过自动化过程从这些文本中生成输入和标签。更准确地说，它被训练来猜测句子中的下一个词。
具体而言，输入是一定长度的连续文本序列，目标是相同的序列，向右移动一个标记（单词或词片段）。模型在内部使用掩码机制，以确保对于标记i的预测仅使用从1到i的输入，而不使用未来的标记。通过这种方式，模型学习到了英语语言的内部表示，这可以用来提取对下游任务有用的特征。然而，该模型在其预训练任务上表现最佳，即根据提示生成文本。

GPT-2的核心思想概括为：任何有监督任务都是语言模型的一个子集，当模型的容量非常大且数据量足够丰富时，仅仅靠训练语言模型的学习便可以完成其他有监督学习的任务。

GPT-2的模型结构和GPT-1的模型结构类似，都是基于Transformer的。相对于GPT-1，做的修改有：

1. 调整Transformer的decoder： 将归一化层移动到block的输入位置并且在最后一个self-attention之后加了一层归一化。
2. 数据量扩增：GPT1利用了约5GB，GPT2利用了40GB，并且质量更高
3. 词典被扩展到了50257，context的维度从512提高到了1024并且batchsize采用了512。
4. 堆叠的层数增加：GPT1使用的12层的 TransformerDecoder，GPT2分别使用了24、36、48层。
5. **去掉了Fine-tune部分：使用了完全的无监督训练。这样使得预训练和Fine-tuning的结构完全一致。**

总结来说，就是 GPT2 认为只需要做预训练就可以了，只要训练语料足够大，模型足够大，那么就不需要所谓的 funetune了，可以通过类似 prompt 的方式来轻松实现各种任务。现在这个思想已经是主流了。

也就是说 GPT2 只有 text prediction head。

GPT-2的最大贡献是验证了通过海量数据和大量参数训练出来的词向量模型有迁移到其它类别任务中而不需要额外的训练。但是很多实验也表明，GPT-2的无监督学习的能力还有很大的提升空间，甚至在有些任务上的表现不比随机的好。尽管在有些zero-shot的任务上的表现不错，但是我们仍不清楚GPT-2的这种策略究竟能做成什么样子。GPT-2表明随着模型容量和数据量的增大，其潜能还有进一步开发的空间，基于这个思想，诞生了我们下面要介绍的GPT-3。

# GPT3
https://arxiv.org/abs/2005.14165  
Language Models are Few-Shot Learners

虽然 GPT-2 主推的 zero-shot 在创新度上有比较高的水平，但是由于其在效果上表现平平，所以在业界并没有取得比较大的影响力，而 GPT-3 正是为了解决效果上的问题而提出的。GPT-3 不再去追求那种极致的不需要任何样本就可以表现很好的模型，而是考虑像人类的学习方式那样，仅仅使用极少数样本就可以掌握某一个任务，因此就引出了 GPT-3 标题 Language Models are Few-Shot Learners。

这里的 few-shot 不是像之前的方式那样，使用少量样本在下游任务上去做微调，因为在 GPT-3 那样的参数规模下，即使是参数微调的成本也是高到无法估计。他其实是我们现在说的 In-context learning。

<div align=center>
<img src="https://github.com/QwenLM/Qwen-7B/assets/17425982/7cba5316-3f33-41df-ad9c-7644ed933888"/>
</div>

上图解释的非常清楚了。

模型参数如下所示，gpt3 一般指的就是 1750 亿参数的最大模型

<div align=center>
<img src="https://github.com/QwenLM/Qwen-7B/assets/17425982/95c10bcc-50d3-49fc-8269-74eb379821ba"/>
</div>

相比于 gpt2 ，整个结构几乎没有变。但是引入了 Sparse Transformer 中的 sparse attention 模块（稀疏注意力）

使用 sparse attention 的好处主要有以下两点：

1. 减少注意力层的计算复杂度，节约显存和耗时，从而能够处理更长的输入序列；
2. 具有“局部紧密相关和远程稀疏相关”的特性，对于距离较近的上下文关注更多，对于距离较远的上下文关注较少；

关于 sparse attention 详情可参考《Generating Long Sequences with Sparse Transformers》

GPT-3共训练了5个不同的语料，分别是低质量的Common Crawl，高质量的WebText2，Books1，Books2和Wikipedia，GPT-3根据数据集的不同的质量赋予了不同的权值，权值越高的在训练的时候越容易抽样到

<div align=center>
<img src="https://github.com/QwenLM/Qwen-7B/assets/17425982/c2d48c51-e90a-4f0a-8148-646b77f6b29c"/>
</div>

# InstructGPT

Training language models to follow instructions with human feedback   
https://arxiv.org/abs/2203.02155

GPT-3 虽然在各大 NLP 任务以及文本生成的能力上令人惊艳，但是他仍然还是会生成一些带有偏见的，不真实的，有害的造成负面社会影响的信息，而且很多时候，他并不按人类喜欢的表达方式去说话。在这个背景下，OpenAI 提出了一个概念“Alignment”，意思是模型输出与人类真实意图对齐，符合人类偏好。因此，为了让模型输出与用户意图更加 “align”，就有了 InstructGPT 这个工作。

因为语言模型本身就是有很多种表述，虽然现在 GPT3 具备了很强的生成能力，但是他并不知道哪一种表述是人类喜欢的，因此，我们需要一种方式来告诉模型哪一种表述是人类喜欢的，这就是 InstructGPT 的核心思想。

InstructGPT 提出了一个理想化语言模型的三大目标：

- 有用的：模型输出的内容是有用的，能够帮助人类解决问题；
- 真实的：模型输出的内容是真实的，不能误导用户；
- 无害的：模型输出的内容是无害的，不会对用户或者环境造成物理，精神和社会影响；

这个在现在非常重要。比如之前 google 和 facebook 就都出现过把照片中的黑人识别成黑猩猩的事件，然后随之而来的就是产品的整改，公关道歉之类的。

<div align=center>
<img src="https://github.com/QwenLM/Qwen-7B/assets/17425982/41ee5e1e-89ae-4329-9297-194af199a42d"/>
</div>

<div align=center>
<img src="https://github.com/QwenLM/Qwen-7B/assets/17425982/a979003f-0b0a-4c02-9a7d-c5fecc5ed345"/>
</div>

原文分为了三个步骤：有监督微调，奖励模型训练，强化学习训练；实际上可以把它拆分成两种技术方案，一个是有监督微调（SFT），一个是基于人类反馈的强化学习（RLHF）

因此现在来说一个大语言模型一般都要进行4步训练

<div align=center>
<img src="https://github.com/QwenLM/Qwen-7B/assets/17425982/c0722f8c-3d3c-4391-892d-3162c75cc6ae"/>
</div>

**(1) 无监督预训练**

预训练（Pretraining）阶段需要利用海量的训练数据，包括互联网网页、维基百科、书籍、 GitHub、 论文、问答网站等，构建包含数千亿甚至数万亿单词的具有多样性的内容。利用由数千块高性能
GPU 和高速网络组成超级计算机，花费数十天完成深度神经网络参数训练，构建基础语言模型 （Base Model）。基础大模型构建了长文本的建模能力，使得模型具有语言生成能力，根据输入的提示词（Prompt），模型可以生成文本补全句子

这部分训练收集非常多丰富数据。

**(2) 有监督微调**

这里的有监督微调(Supervised Finetuning)，其实叫做指令微调(Instruction Tuning)。一定要分清这里的指令微调和我们之前理解或者和 CV 里面常说的下游任务微调差异很大。

之前的的微调或者说 GPT2 下游微调，会先收集特定任务的数据集，然后在这个数据集上进行微调。而这里的指令微调并不是针对某个或某几个特定任务来收集数据，而是收集大量符合人类喜好的高质量的 <指令, 响应>对，
而不是针对某个任务来说。这样训练的模型可以具备 prompt 能力，即使微调时候没有见过这个任务，根据指令也可以输出符合人类喜好的响应。

只有这样做才有意义。因为用户在向 GPT-3 提问时不会采用某种固定的任务表述，而是随心所欲地以自己的说话习惯去表达某个需求。InstructGPT 在 SFT 中标注的数据，正是为了消除这种模型预测与用户表达习惯之间的 gap。在标注过程中，他们从 GPT-3 的用户真实请求中采样大量下游任务的描述，然后让标注人员对任务描述进行续写，从而得到该问题的高质量回答。这里用户真实请求又被称为某个任务的指令，即 InstructGPT 的核心思想“基于人类反馈的指令微调”。

利用这些有监督数据，使用与预训练阶段相同的语言模型训练算法，在基础语言模型基础上再进行训练，从而得到有监督微调模型（SFT 模型）。经过训练的 SFT 模型具备了初步的指令理解能力和上下文理解能力，能够完成开放领域问题、阅读理解、翻译、生成代码等能力，也具备了一定
的对未知任务的泛化能力。由于有监督微调阶段的所需的训练语料数量较少， SFT 模型的训练过程并不需要消耗非常大量的计算。

很多类 ChatGPT 的模型都属于该类型，包括： Alpaca、 Vicuna、 MOSS、 ChatGLM-6B 等。很多这类模型效果也非常好，甚至在一些评测中达到了 ChatGPT 的 90% 的效果。
当前的一些研究表明有监督微调阶段数据选择对 SFT 模型效果有非常大的影响，因此如何构造少量并且高质量的训练数据是本阶段有监督微调阶段的研究重点。

**(3) 奖励模型训练**

基于人类反馈的强化学习（RLHF）需要先学习奖励模型。

奖励建模（Reward Modeling）阶段目标是构建一个文本质量对比模型，对于同一个提示词， SFT 模型给出的多个不同输出结果的质量进行排序。奖励模型（RM 模型）可以通过二分类模型，对输
入的两个结果之间的优劣进行判断。 RM 模型与基础语言模型和 SFT 模型不同， RM 模型本身并不能单独提供给用户使用。奖励模型的训练通常和 SFT 模型一样，使用数十块 GPU，通过几天时
间完成训练。由于 RM 模型的准确率对于强化学习阶段的效果有着至关重要的影响，因此对于该模型的训练通常需要大规模的训练数据。 Andrej Karpathy 在报告中指出，该部分需要百万量级的对比数据标注，而且其中很多标注需要花费非常长的时间才能完成。

RM 可以是一个2分类模型，也可以是一个打分模型，对多个输入进行打分，从而得到一个排序好的数据集。这个数据的标注需要人工标注，或者通过网站来不断收集。

奖励模型是一个独立的模型，可以和前面的模型不一样，但是目前来看，规模上也有要求，不能太少。

**(4) 基于人类反馈的强化学习**

以 Reward Model 作为奖励函数，以 SFT 为初始策略，跑 PPO 算法改善策略表现，得到强化版本的模型，这个版本也就是所谓的InstructGPT。

关于强化学习部分以后再说。

总的来说，InstructGPT 相对于之前的 GPT 系列，有以下几点值得注意：

1. 解决 GPT-3 的输出与人类意图之间的 Align 问题；
2. 让具备丰富世界知识的大模型，学习“人类偏好”；
3. 标注人员明显感觉 InstructGPT 的输出比 GPT-3 的输出更好，更可靠；
4. InstructGPT 在真实性，丰富度上表现更好；
5. InstructGPT 对有害结果的生成控制的更好，但是对于“偏见”没有明显改善；
6. 基于指令微调后，在公开任务测试集上的表现仍然良好；
7. InstructGPT 有令人意外的泛化性，在缺乏人类指令数据的任务上也表现很好；

InstructGPT仍然会犯非常简单的错误，如不服从指令，捏造事实，简单的问题规避直接的回答导致回应又臭又长，难以检测带有假前提的指令，如“为什么贾宝玉三顾茅庐请孙悟空出山？”

# 拆解追溯 GPT-3.5 各项能力的起源

核心观点整理。

初代GPT-3展示了三个重要能力：

- **语言生成**：遵循提示词（prompt），然后生成补全提示词的句子 (completion)。这也是今天人类与语言模型最普遍的交互方式。
- **上下文学习 (in-context learning)**:  遵循给定任务的几个示例，然后为新的测试用例生成解决方案。很重要的一点是，GPT-3虽然是个语言模型，但它的论文几乎没有谈到“语言建模” (language modeling) —— 作者将他们全部的写作精力都投入到了对上下文学习的愿景上，这才是 GPT-3的真正重点。
- **世界知识 (world knowledge)**：包括事实性知识 (factual knowledge) 和常识 (commonsense)。

其中
- **语言生成**的能力来自于语言建模的**训练目标** (language modeling)。
- **世界知识**来自 3000 亿单词的**训练语料库**（不然还能是哪儿呢）。
- **模型的 1750 亿参数**是为了**存储知识**，Liang et al. (2022) 的文章进一步证明了这一点。 他们的结论是，知识密集型任务的性能与模型大小息息相关。
- 上下文学习的能力来源及为什么上下文学习可以泛化，**仍然难以溯源。

进化树：

<div align=center>
<img src="https://github.com/QwenLM/Qwen-7B/assets/17425982/ead38bad-e9cc-4465-9864-db10ea923153"/>
</div>

- 指令微调**不会为模型注入新的能力** —— 所有的能力都已经存在了。指令微调的作用是**解锁 / 激发这些能力**。这主要是因为指令微调的数据量比预训练数据量少几个数量级（基础的能力是通过预训练注入的）。
- 指令微调**将 GPT-3.5 的分化到不同的技能树。**有些更擅长上下文学习，如`text-davinci-003`，有些更擅长对话，如`ChatGPT`。
- 指令微调**通过牺牲性能换取与人类的对齐（alignment）**。 OpenAI 的作者在他们的指令微调论文中称其为 “对齐税” (alignment tax)。许多论文都报道了`code-davinci-002`在基准测试中实现了最佳性能（但模型不一定符合人类期望）。 在`code-davinci-002`上进行指令微调后，模型可以生成更加符合人类期待的反馈（或者说模型与人类对齐），例如：零样本问答、生成安全和公正的对话回复、拒绝超出模型它知识范围的问题。

RLHF 触发的能力：

- **翔实的回应：** text-davinci-003 的生成通常比 text-davinci-002长。 ChatGPT 的回应则更加冗长，以至于用户必须明确要求“用一句话回答我”，才能得到更加简洁的回答。这是 RLHF 的直接产物。
- **公正的回应：**ChatGPT 通常对涉及多个实体利益的事件（例如政治事件）给出非常平衡的回答。这也是RLHF的产物。
- **拒绝不当问题：**这是内容过滤器和由 RLHF 触发的模型自身能力的结合，过滤器过滤掉一部分，然后模型再拒绝一部分。
- **拒绝其知识范围之外的问题：**例如，拒绝在2021 年 6 月之后发生的新事件（因为它没在这之后的数据上训练过）。这是 RLHF 最神奇的部分，因为它使模型能够隐式地区分哪些问题在其知识范围内，哪些问题不在其知识范围内。

我们可以得出结论：

- 语言生成能力 + 基础世界知识 + 上下文学习都是来自于预训练（`davinci`）
- 存储大量知识的能力来自 1750 亿的参数量。
- 遵循指令和泛化到新任务的能力来自于扩大指令学习中指令的数量（`Davinci-instruct-beta`)
- 执行复杂推理的能力很可能来自于代码训练（`code-davinci-002`）
- 生成中立、客观的能力、安全和翔实的答案来自与人类的对齐。具体来说：
    - 如果是监督学习版，得到的模型是`text-davinci-002`
    - 如果是强化学习版 (RLHF) ，得到的模型是`text-davinci-003`
    - 无论是有监督还是 RLHF ，模型在很多任务的性能都无法超过 code-davinci-002 ，这种因为对齐而造成性能衰退的现象叫做对齐税。
- 对话能力也来自于 RLHF（`ChatGPT`），具体来说它牺牲了上下文学习的能力，来换取：
    - 建模对话历史
    - 增加对话信息量
    - 拒绝模型知识范围之外的问题


