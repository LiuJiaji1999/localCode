## 具身智能 Embodied AI

### 领域实践
```bash
自动驾驶领域：VLA、VLN，强化学习，模仿学习
1.数据驱动是端到端自动驾驶的核心支撑力量。根据学习原理划分，端到端自动驾驶系统可分为：
    ·强化学习方法依赖在数以百万计的试错场景中训练模型，使其自主学习并掌握加减速、转向、变道等驾驶技能，对数据规模与计算能力的要求极高。
    ·模仿学习方法则通过大规模标注数据和真实驾驶样本对模型进行监督训练，让算法快速学习人类驾驶经验，更适合在量产车场景中快速迭代。

2.当前智能驾驶已分化出两条路径。2024 年夏季以来，端到端(End-to-End)智能驾驶技术格局发生显著变化，从 2023年“一家独大” 的单一技术路径，正式演进为两大核心路线并行发展的态势。VLA 和世界模型并不是完全对立，而是两条目前并行发展的技术路线。其中，
`第一条路线为以“视觉 - 语言 - 行为”(Visual-Language-Action,VLA)链路为核心的架构体系，VLA 适合快速选代兼容现有量产平台，短期内易于落地。
    VLA架构的技术逻辑遵循 “感知-理解-决策-控制” 的全链路语义化转化，其核心流程可拆解为四步:
    ·首先通过图像传感器完成环境感知与图像识别；
    ·其次将视觉感知结果转化为可被大语言模型理解的语言 Token(如“前方 50 米有静止车辆”“当前车道为潮汐车道”);
    ·再由大模型基于语言 Token 生成符合人类驾驶逻辑的驾驶建议；
    ·最后将抽象建议转化为 具体的车辆控制轨迹，并通过实时反馈机制进行动态校正。
`第二条路线则是以物理推演为核心驱动力的世界模型(World Model)路线，世界模型则代表了更底层的认知方式，强调物理规律和空间理解力，适合长期演进。
    与VLA架构依赖“语言中间层”的技术逻辑不同，世界模型路线通过绕开语言转化环节，直接将 3D高斯表征、点云等空间感知教据输入大模型，在模型潜空间内完成物理规律推演(如车辆碰撞风险预判、行人运动轨迹预测)，并直接输出车辆控制指令，形成 云端世界引擎 + 车端世界行为模型 的技术架构:
    ·云端世界引擎负责大规模场景的物理规律建模与预训练，
    ·车端世界行为模型则基于实时感知数据进行快速决策。

----------------------------------------------------------------

电力巡检领域：人形机器人、《电力具身智能机器人发展白皮书》
```

### 强化学习 Reinforcement Learning
```bash
mathworks公司：https://www.mathworks.com/videos/series/reinforcement-learning.html

agent-> reward action observation-> environment
Agents学习：https://datawhalechina.github.io/hello-agents/#/

RL理解：https://mp.weixin.qq.com/s/Ukt2a1IC4sVbsGCLemk3xQ，https://lei-kun.github.io/blogs/rl.html
可通过两个问题理解：
1.数据从哪里来？
    RL本质上是智能体不断收集经验、并用这些经验改进策略的循环。不同算法的差异，很大程度上取决于它们依赖什么样的数据。
    ·最直接：在线策略学习。在这种模式下，智能体一边与环境交互，一边学习。
            每一个动作都带来新的数据，立刻被用于更新模型。这类方法像是不断在现场实践的学生，代表算法包括 PPO、SAC 等。
            在线学习的优点是灵活、适应性强，但也意味着代价高昂，每次试错都可能耗费时间、能量，甚至造成损失。
    ·相对保守：离线策略学习。它允许智能体反复使用过去的经验，而不必每次都重新与环境交互。
            算法会把这些经验保存下来，在需要时反复采样学习。DQN、TD3、DDPG 都属于这一类。
            离策略学习提高了样本利用率，也让学习过程更稳定，是许多实际应用中的主流方案。
    ·极端方法：离线学习。智能体完全依赖一个固定的数据集进行训练，不能再与环境交互。这种方法看似受限，但在高风险场景中却尤为重要，比如医疗、自动驾驶或机器人控制。
                算法必须在不试错的情况下，从既有数据中学会尽可能好的决策，CQL、IQL 就是这类方法的代表。
&& 从在线到离线，数据的获取方式逐渐从主动探索转向被动利用。算法的选择往往反映了任务的现实约束：能否安全地试错？能否持续获得新数据？试错的代价是否可承受？这便是强化学习的第一个维度：数据从哪里来。
2.策略怎么更新？
    学习更新的节奏。简单来说，就是智能体多久评估一次策略，又多久调整一次行为。
    ·最简单：一步式学习。智能体在一个固定的数据集上训练一次，学到一个策略后就不再改进。
            模仿学习就是典型例子。它速度快、风险低，适合那些对安全性要求高或数据有限的任务。
    ·另一种：多步式学习。算法在一批数据上多次更新，直到性能收敛，再重新收集新的数据。
            这是一种折中策略，既避免了频繁交互的高成本，又能比一次性训练获得更好的表现。
    ·最具代表：迭代式学习。这类算法不断在“收集数据—更新模型—再收集数据”的循环中进化，每一次交互都推动性能提升。
            它们像一个永不停歇的学习者，不断探索未知、修正自身。PPO 和 SAC 就是这种方式的代表。
&& 从一步到多步，再到迭代，算法的更新节奏越来越密集，也意味着从静态到动态的转变。不同节奏之间，其实反映的是对稳定性和适应性的权衡。
3.所有RL都在做上述两件事，就像一个反复自我练习的过程
    先评估，看看自己目前的策略表现得怎样，哪些动作好、哪些不好；
        评估阶段（Policy Evaluation） 就是去衡量“这套策略到底值不值”。
        算法会让模型预测某个状态下采取某个动作能得到多大的回报，然后和实际反馈进行比较。如果误差太大，就调整模型，让它的预期更接近现实。
        比如在线算法直接用新数据计算，离线或离策略算法则要通过重要性采样、加权平均等方式修正旧数据的偏差。
    后改进，根据评估结果，调整策略，让下一次决策更聪明一点。
        改进阶段（Policy Improvement） 是在得到新的评估结果后，优化策略本身。
        模型会倾向于选择那些带来更高期望回报的动作。但为了避免一下子“改过头”，很多算法会加上约束或正则项，
        比如让新策略不能偏离旧策略太多（这就是 PPO 的思想），或者在策略里保留一定的探索性（这就是 SAC 中熵正则的作用）。
&& Q-learning、PPO、SAC……看起来名字各不相同，其实都在重复这两个动作。唯一的区别，只是它们评估得方式不同、改进的速度不同、或者用到的数据不同。
```
![Alt text](evaluation-improvement.png)