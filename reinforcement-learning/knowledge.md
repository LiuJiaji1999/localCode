## 具身智能 Embodied AI
```bash

```

### 自动驾驶 Autonomous Driving
```bash
https://github.com/JohnsonJiang1996/Awesome-VLA4AD?tab=readme-ov-file

1. AD技术发展路线
·端到端自动驾驶 (End-to-End AD): 这种模式将传感器输入直接映射到驾驶动作，省去了复杂的中间模块。虽然高效，但其“黑箱”特性导致可解释性差，难以处理需要高级推理的“长尾”场景。数据驱动是端到端自动驾驶的核心支撑力量，根据学习原理划分，可分为：
    强化学习方法： 依赖在数以百万计的试错场景中训练模型，使其自主学习并掌握加减速、转向、变道等驾驶技能，对数据规模与计算能力的要求极高。
    模仿学习方法： 则通过大规模标注数据和真实驾驶样本对模型进行监督训练，让算法快速学习人类驾驶经验，更适合在量产车场景中快速迭代。
# 架构： 环境信息输入 → 端到端网络 → 驾驶动作。

·用于自动驾驶的视觉语言模型 (VLMs for AD): 随着大语言模型（LLM）的兴起，研究者开始将语言的理解和推理能力引入自动驾驶。VLM能够解释复杂的交通场景、回答相关问题，显著提升了系统的可解释性和对罕见事件的泛化能力。然而，这些模型主要停留在“感知和理解”，语言输出与车辆的实际控制脱节，存在“行动鸿沟”。
# 架构： 环境信息输入 → VLM → 推理链/多任务 → 输出（非直接控制）。

·用于自动驾驶的视觉-语言-行动模型 (VLA for AD): VLA模型是当前最前沿的范式。它在一个统一的策略中融合了视觉感知、语言理解和动作执行。VLA旨在打造能够理解高级指令、推理复杂场景并自主决策的智能车辆。VLA模型不仅能遵循“让行救护车”这类自然语言指令，还能用语言解释其决策原因，实现了感知、推理和行动的闭环。
# 架构： 环境信息输入 → 多模态编码器 → LLM/VLM → 动作解码器 → 驾驶动作。👇
```
![Alt text](./image/ad.png)

```bash
2. 当前智能驾驶已分化出两条路径。2024 年夏季以来，End-to-End 智能驾驶技术格局发生显著变化，从 2023年“一家独大” 的单一技术路径，正式演进为两大核心路线并行发展的态势。VLA 和世界模型并不是完全对立，而是两条目前并行发展的技术路线。其中，

1️⃣ 第一条路线为以“视觉 - 语言 - 行为”(Visual-Language-Action,VLA)链路为核心的架构体系，VLA 适合快速选代兼容现有量产平台，短期内易于落地。
1）VLA架构的技术逻辑遵循 “感知-理解-决策-控制” 的全链路语义化转化，其核心流程可拆解为四步:
  ·首先通过图像传感器完成环境感知与图像识别；
  ·其次将视觉感知结果转化为可被大语言模型理解的语言 Token(如“前方 50 米有静止车辆”“当前车道为潮汐车道”);
  ·再由大模型基于语言 Token 生成符合人类驾驶逻辑的驾驶建议；
  ·最后将抽象建议转化为 具体的车辆控制轨迹，并通过实时反馈机制进行动态校正。
```
![Alt text](./image/vla4ad.png)
```bash
# 架构： 环境信息输入 → 多模态编码器 → LLM/VLM → 动作解码器 → 驾驶动作。
a.多模态输入与语言指令 (Multimodal Inputs and Language Commands)
    视觉数据 (Visual Data)： 视觉是自动驾驶系统的核心输入。技术已从早期的单前视摄像头发展到如今的多摄像头环视系统。原始图像可以被直接处理，或转换为鸟瞰图（BEV）等结构化表示，以辅助空间推理。
    其他传感器数据 (Other Sensor Data)： 为增强空间感知能力，系统还融合了多种传感器。包括用于精确3D结构的激光雷达（LiDAR）、用于速度估计的雷达（RADAR）、用于运动追踪的惯性测量单元（IMU）以及用于全局定位的GPS。方向盘转角、油门等本体感知数据也愈发重要。
    语言输入 (Language Inputs)： 语言输入的形式日趋丰富，其演进路径如下：
        直接导航指令：例如“在下一个路口左转”。
        环境查询：例如“现在变道安全吗？”。
        任务级指令：例如，用自然语言解析交通规则或高阶目标。
        对话式推理：最新的研究已支持多轮对话和基于思维链（CoT）的复杂推理，甚至包括语音指令输入。
b.核心架构模块 (Core Architectural Modules)
    视觉编码器 (Vision Encoder)： 该模块负责将原始图像和传感器数据转换为潜在表征。
        通常使用如 DINOv2 或 CLIP 等大型自监督模型作为骨干网络。🚩<Model1> 
        许多系统采用 BEV投影技术，或通过 点云编码器（如PointVLA）来融合3D信息。🚩<Model2>
    语言处理器 (Language Processor)： 该模块使用预训练的语言模型（如LLaMA2或GPT系列）来处理自然语言指令。
        通过 指令微调 或 低秩适应LoRA 等轻量化微调策略，可以高效地让模型适应自动驾驶领域的特定知识。🚩<Model3>
    动作解码器 (Action Decoder)： 该模块负责生成最终的控制输出。其实现方式主要有三种：
        自回归令牌器：将连续的轨迹点或离散的动作（如“加速”、“左转”）作为Token，并依次生成。
        扩散模型头 (Diffusion heads)：基于融合后的特征，通过采样生成连续的控制信号。🚩<Model4>
        分层控制器：由一个高阶的语言规划器生成子目标（如“超车”），再由一个低阶的比例-积分-微分控制器（Proportion Integral Differential，PID）、 模型预测控制（MPC）等车辆控制器来执行具体轨迹。
        【多Model的输出 -> controler的输入-> 👇】
c.驾驶输出 (Driving Outputs), VLA模型的输出形式反映了其抽象层次和操作目标
    低阶动作 (Low-Level Actions)： 一部分VLA4AD系统直接预测原始控制信号，如方向盘转角、油门和刹车。
        这种方式优点是可以输出更精细的控制，但对感知误差敏感，且缺乏长远规划能力，并且不同车型的可拓展性较差。
    轨迹规划 (Trajectory Planning)： 另一些VLA自动驾驶研究输出预测轨迹或路径点。
        这种方式具有更好的可解释性和拓展能力，可以由下游的MPC等规划器灵活执行。它使得VLA模型能够进行更长时程的推理，并更有效地整合多模态信息。
d.代表性工作、数据基准
    📑《A Survey on Vision-Language-Action Models for Autonomous Driving》

2）技术路线核心特征和痛点问题
技术落地：3D中间表征、长时序记忆、多模态融合（视觉、运动、语义等多源信息
Q1 多源数据的时序同步与时空一致性: VLA模型的有效运行依赖多源信息(如摄像头、雷达、语音指令)的时序同步与时空对齐。
    但在实际工程中，不同传感器的采集频率和延迟存在天然差异，如视觉帧率高、语音输入低频，导致特征对齐时易出现时间错位和空间漂移，严重时可能干扰下游决策逻辑。
    为此，产业界正在探索基于时空对齐优化的跨模态特征融合策略，如引入动态缓存机制、模态对齐层或中间对齐表征，以缓解多源异步问题。但这一过程对系统架构设计、算力调度和算法鲁棒性均提出更高要求。
Q2 极端工况下的模型稳定性: 在暴雨、强光、隧道明暗突变等极端环境下，感知模块性能普遍出现显著衰减。
    其技术难点在于如何实现模态间的动态互补机制？
        例如在视觉传感受限时通过雷达与语言引导实现信息补强，
        或在语音识别不稳定时通过视觉和地图信息反向约束决策逻辑；
        同时，如何在复杂工况下保证模态间的置信度判断与自适应权重调整；
Q3 长尾场景的泛化能力: 对长尾低频场景的泛化，例如夜间施工、动物横穿马路等，这些情境往往样本稀缺且分布复杂。
    当前业界普遍通过预训练与迁移学习、语义分解等方式实现视觉一语言的动态耦合，以提高语义映射的准确性与泛化性。
    然而在实际工程落地中仍面临标注噪声、语义漂移和场景偏移等系统性挑战，成为制约大规模落地的核心痛点之一。


2️⃣ 第二条路线则是以物理推演为核心驱动力的世界模型(World Model)路线，世界模型则代表了更底层的认知方式，强调物理规律和空间理解力，适合长期演进。
1) 与VLA架构依赖“语言中间层”的技术逻辑不同，世界模型路线通过绕开语言转化环节，直接将 3D高斯表征、点云等空间感知教据输入大模型，在模型潜空间内完成物理规律推演(如车辆碰撞风险预判、行人运动轨迹预测)，并直接输出车辆控制指令，形成 云端世界引擎 + 车端世界行为模型 的技术架构:
    ·云端世界引擎负责大规模场景的物理规律建模与预训练，
    ·车端世界行为模型则基于实时感知数据进行快速决策。

2）技术研究：
🔔Marble渲染「世界长什么样」，Genie3展示「世界怎么变」，JEPA则探究「世界的结构是什么」。
Li Fei-Fei, 世界模型即界面
    以Marble 为代表，它让人们能够从文字或二维素材，直接生成可编辑、可分享的三维环境。
    这种模式下，「世界」是呈现在VR头显、显示器或电脑屏幕上的那片可供人观看与游走的空间。 3D高斯泼溅
Google, 世界模型即模拟器
    以Genie3 为代表，这类模型能生成连续、可控制的视频式世界，让智能体在其中反复尝试、失败、再尝试。
    像SIMA2 这样的智能体，便可把这类世界当作「虚拟健身房」。
Yann LeCun, 世界模型即认知框架
    以JEPA 为代表，这是一种高度抽象的形式，没有像前两种一样可供人欣赏的画面。
    在这里，关注点不在于渲染，「世界」以潜在变量和状态转移函数的形式呈现，可以说是机器人完美的训练基地。


📑 Yann LeCun, Li Fei-Fei, Saining Xie 等《Cambrian-S Towards Spatial Supersensing in Video》2511
📑 

🚗 ➡️ 理解道路+作出驾驶动作
```


### 人形机器人 Humanoid Robot
```bash
1️⃣ knowledge/PRCV2024-大模型驱动的具身智能人形机器人（湖南大学-王耀南）.pdf

2️⃣ 技术发展
·VLA技术的应用，同上👆
·与AD的区别：
  控制难度：机器人的行走、保持平衡等动作，控制难度高于汽车；
  感知：主要为RGB、深度、触觉、力矩、指尖传感器等特殊传感器，需要更强的几何和物理理解；
  执行任务类型：抓取、灭火等复杂任务，强依赖物理交互，侧重“操作与交互”；
  语言能力：精准理解指令，意图推理，多步规划；
  实时：容错率略高；
  数据来源：仿真模拟器大规模生成，Sim2Real难；👇
🚩<Data1>
·可自建的仿真模拟器: Unity3D引擎  -https://unity.cn
                  Blender -https://www.blender.org/download/, 
·已有的仿真/Control平台:
a.无人机飞行 
    AirSim -https://microsoft.github.io/AirSim/ 
    PX4_Gazebo -https://zhuanlan.zhihu.com/p/337919677
b.机器人控制系统 
    ROS2: https://www.ros.org/blog/getting-started/#  可模拟navigation stack、move_base、collision avoidance等
·其他关于Vision-language的公开研究数据集
    EGO-EXO4D：https://ego-exo4d-data.org/#
    LEMMA：https://sites.google.com/view/lemma-activity

🤖 ➡️ 理解世界+执行复杂操作动作

电力巡检项目：
📑《电力具身智能机器人发展白皮书》
1. 现状机遇
Q:可靠性、通过性、通用性、协同性
O:内生需求、技术发展、政策引领、市场规模
2. 关键技术体系
围绕：模型赋予本体智能、本体与环境交互产生数据、数据用于模型训练，
～机器人本体：
    大量通用+少量专用
    突破复杂环境下长期稳定生存的本体、仿生机器人（耐高温可灵活）、步态规划、多模态传感器等
～数据：
    专用真机数据+通用数据，虚实结合具身数据
    ①穿戴设备采集大量人体作业数据，完成对任务执行流程的理解；
    ②构造适量的仿真合成数据，模拟场景作业中的长尾任务；
    ③利用少量的真机数据，用于模型微调。
～模型：
    专用小模型+通用大模型 
    突破多模态感知、异构协同、云-网-端通信等
3. 典型应用
    电力专用机器人、输电线路上塔作业机器人、空地协同异构智能勘灾抢修集群等

📑《DetectiumFire： A Comprehensive Multi-modal Dataset for Fire Understanding》NeurIPS2025
📑
```
----------------------------------------------------------------

### 强化学习 Reinforcement Learning
```bash
mathworks公司：https://www.mathworks.com/videos/series/reinforcement-learning.html

agent-> reward action observation-> environment
Agents学习：https://datawhalechina.github.io/hello-agents/#/

RL理解：https://mp.weixin.qq.com/s/Ukt2a1IC4sVbsGCLemk3xQ，https://lei-kun.github.io/blogs/rl.html
可通过两个问题理解：
1.数据从哪里来？
    RL本质上是智能体不断收集经验、并用这些经验改进策略的循环。不同算法的差异，很大程度上取决于它们依赖什么样的数据。
    ·最直接：在线策略学习。在这种模式下，智能体一边与环境交互，一边学习。
            每一个动作都带来新的数据，立刻被用于更新模型。这类方法像是不断在现场实践的学生，代表算法包括 PPO、SAC 等。
            在线学习的优点是灵活、适应性强，但也意味着代价高昂，每次试错都可能耗费时间、能量，甚至造成损失。
    ·相对保守：离线策略学习。它允许智能体反复使用过去的经验，而不必每次都重新与环境交互。
            算法会把这些经验保存下来，在需要时反复采样学习。DQN、TD3、DDPG 都属于这一类。
            离策略学习提高了样本利用率，也让学习过程更稳定，是许多实际应用中的主流方案。
    ·极端方法：离线学习。智能体完全依赖一个固定的数据集进行训练，不能再与环境交互。这种方法看似受限，但在高风险场景中却尤为重要，比如医疗、自动驾驶或机器人控制。
                算法必须在不试错的情况下，从既有数据中学会尽可能好的决策，CQL、IQL 就是这类方法的代表。
&& 从在线到离线，数据的获取方式逐渐从主动探索转向被动利用。算法的选择往往反映了任务的现实约束：能否安全地试错？能否持续获得新数据？试错的代价是否可承受？这便是强化学习的第一个维度：数据从哪里来。
2.策略怎么更新？
    学习更新的节奏。简单来说，就是智能体多久评估一次策略，又多久调整一次行为。
    ·最简单：一步式学习。智能体在一个固定的数据集上训练一次，学到一个策略后就不再改进。
            模仿学习就是典型例子。它速度快、风险低，适合那些对安全性要求高或数据有限的任务。
    ·另一种：多步式学习。算法在一批数据上多次更新，直到性能收敛，再重新收集新的数据。
            这是一种折中策略，既避免了频繁交互的高成本，又能比一次性训练获得更好的表现。
    ·最具代表：迭代式学习。这类算法不断在“收集数据—更新模型—再收集数据”的循环中进化，每一次交互都推动性能提升。
            它们像一个永不停歇的学习者，不断探索未知、修正自身。PPO 和 SAC 就是这种方式的代表。
&& 从一步到多步，再到迭代，算法的更新节奏越来越密集，也意味着从静态到动态的转变。不同节奏之间，其实反映的是对稳定性和适应性的权衡。
3.所有RL都在做上述两件事，就像一个反复自我练习的过程
    先评估，看看自己目前的策略表现得怎样，哪些动作好、哪些不好；
        评估阶段（Policy Evaluation） 就是去衡量“这套策略到底值不值”。
        算法会让模型预测某个状态下采取某个动作能得到多大的回报，然后和实际反馈进行比较。如果误差太大，就调整模型，让它的预期更接近现实。
        比如在线算法直接用新数据计算，离线或离策略算法则要通过重要性采样、加权平均等方式修正旧数据的偏差。
    后改进，根据评估结果，调整策略，让下一次决策更聪明一点。
        改进阶段（Policy Improvement） 是在得到新的评估结果后，优化策略本身。
        模型会倾向于选择那些带来更高期望回报的动作。但为了避免一下子“改过头”，很多算法会加上约束或正则项，
        比如让新策略不能偏离旧策略太多（这就是 PPO 的思想），或者在策略里保留一定的探索性（这就是 SAC 中熵正则的作用）。
&& Q-learning、PPO、SAC……看起来名字各不相同，其实都在重复这两个动作。唯一的区别，只是它们评估得方式不同、改进的速度不同、或者用到的数据不同。
```
![Alt text](./image/evaluation-improvement.png)